# Project: Show Me The Data Structures

## Problem1
  The cache is implemented by using dictionary data structure for constant time 
look up and doubly linked list for keeping track of the order of recently used 
cached value as well as constant time remove. A dictionary is used as a mapping of 
cached keys and cached nodes. In addition to actual cached value, each node also 
has two other reference fields, `prev` and `next` that store the cached key of 
the previous (less recently used) and the next (more recently used) cached values, 
respectively.
  Every time a cache is accessed whether by `get()` or `set()` methods, LRU cache 
updates its reference fields to relocate it as the most recently used cached value 
(at the tail of the doubly linked list). When setting a new value that exceeds cache's 
capacity, the LRU cache removes the lease recently used value by detaching the head of 
the doubly linked list. By laveraging the dictionary data type, the LRU cache's Big O 
Notation for insertion, search and deletion is O(1) time complexity.

## Problem2
  The `find_files` function traverses the given path to find all the files with the 
given suffix. We check each entry by using `isdir` and `isfile` from the Python `os` module. 
If we find a directory in the given path, the `find_files` function is called recursively 
with that directory path as parameter. If we find a file that its suffix matches the 
given suffix, we add it to the result. The returned list of paths is a set data 
structure to easily combine high-level and low-level directories using `union` method.
  We have the input as a collection of entries size n, with each entry is a file or a 
directory. The larger the input, the more entries we have to traverse, hence the Big O 
Notation for implemented function is O(n).

## Problem3
  Part 1: Build the Huffman tree. The `heapq` module is used in the implementation of 
Huffman coding to efficiently manage a priority queue. The choice of `heapq` ensure that 
the nodes with the lowest frequencies are efficiently accessed by comparing nodes' 
frequencies (with custom `__lt__` method). Its `heappop` and `heappush` methods allow 
us to easily pop the smallest element as well as pushing new elements onto the heap.
  `build_frequency_dict` has O(n) time complexity since it has to iterate through each 
character in the input, where n is the length of the input data, or the length of the 
input string. `build_huffman_tree` has O(nlogn) time complexity, where n is the number 
of unique characters in the input data. `generate_huffman_codes` has O(n) time complexity 
since each node is visited once when building the tree, where n is the number of nodes.
  Part 2: Encode and decode data. `huffman_encoding` has O(nlogn) time complexity, 
sum of `build_frequency_dict`'s O(m) and `build_huffman_tree`'s O(nlogn). As for decoding, 
`huffman_decoding` has O(n) time complexity, where n is the length of the encoded data.
  Space complexity is O(n) for the priority queue and O(h) for the Huffman tree, where n 
is the size of the queue and h is the height of the tree.

## Problem4
  The `is_user_in_group` function recursively traverses the given group to find if the 
given user is in that group (or group's sub group) or not. The solution uses the `in` 
operator to look in the Python list, hence the Big O Notation is O(n) in the worst case.

## Problem5
  The block chain is implemented as a linked list data structure for constant time 
appending new block to the block chain. Newly created block is always added to the head 
of the linked list. Each block also carries an index as its index in the chain. The 
space complexity is O(n), as new block will take one space in the memory. The more blocks 
we have, the more memory spaces we use.

## Problem6
  To find the union and intersection of two linked lists, we laverage the `union` and 
`intersection` methods of Python set. We take two linked lists, turn them into sets, get 
the union/intersection set of them then turn that set back to linked list. We modify the 
code template to use doubly linked list instead of the given singly linked list to have 
constant time implementation of the append method.
  Every time we turn the given linked list to a set, it takes O(n) time complexity. 
Next, referenced from https://wiki.python.org/moin/TimeComplexity, `union` and `intersection`
of Python set has `O(len(s) + len(t))` and `O(len(s) * len(t))` respectively, worst case.
Finally, our `to_linked_list` has O(n) time complexity. Therefore the whole Big O Notation 
of the solution is O(n), the more elements in the linked lists, the longer it takes to get 
the desired result. The space complexity is also O(n).
